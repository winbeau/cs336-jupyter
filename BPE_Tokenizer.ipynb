{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30b8381-b67f-412c-8677-61971bd0930d",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "- 加载已经训练好的词表`vocab.json`、合并规则`merges.txt`\n",
    "- bpe函数 | 对陌生 str 通过已经训练的规则进行处理\n",
    "- 编码文本 (string -> token IDs)\n",
    "- 解码tokens (token IDs -> string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec721638-7428-4be1-a3a8-e5c50003cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex as re\n",
    "from typing import Dict, List, Tuple, Iterable, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c89efc-f650-4d2f-9a7a-68a9b5c009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Dict[int, bytes],\n",
    "        merges: List[Tuple[bytes, bytes]],\n",
    "        special_tokens: List[str] | None = None,\n",
    "    ):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_rev = {v: k for k, v in vocab.items()}\n",
    "        self.special_tokens = special_tokens or []\n",
    "\n",
    "        # 编译正则\n",
    "        self.PAT = re.compile(r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "        # 预建 merge rank 表\n",
    "        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(\n",
    "        cls,\n",
    "        vocab_path: str,\n",
    "        merges_path: str,\n",
    "        special_tokens: List[str] | None = None,\n",
    "    ) -> \"Tokenizer\":\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            vocab_data = json.load(f)\n",
    "        vocab = {int(i): v.encode(\"utf-8\") for i, v in vocab_data.items()}\n",
    "\n",
    "        merges = []\n",
    "        with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # 忽略注释和非法行\n",
    "                if not line or line.startswith(\"#\"):\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) != 2:\n",
    "                    continue  # 跳过不合规的行\n",
    "                a, b = parts\n",
    "                merges.append((a.encode(), b.encode()))\n",
    "\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    def get_pairs(self, tokens: List[bytes]): # 取相邻 token pair\n",
    "        return {(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)}\n",
    "\n",
    "    def bpe(self, token: bytes) -> List[bytes]: # 执行 Byte-Pair Encoding\n",
    "        word = [bytes([b]) for b in token]  # byte-level 初始分词\n",
    "        pairs = self.get_pairs(word)\n",
    "\n",
    "        while pairs:# 找到当前可合并的最小 rank(最高频率)\n",
    "            min_pair = min(\n",
    "                pairs, key=lambda p: self.bpe_ranks.get(p, float(\"inf\"))\n",
    "            )\n",
    "            if min_pair not in self.bpe_ranks:\n",
    "                break\n",
    "\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if (\n",
    "                    i < len(word) - 1\n",
    "                    and word[i] == min_pair[0]\n",
    "                    and word[i + 1] == min_pair[1]\n",
    "                ):\n",
    "                    new_word.append(word[i] + word[i + 1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            word = new_word\n",
    "            pairs = self.get_pairs(word)\n",
    "\n",
    "        return word\n",
    "        \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Encode text → token ids\"\"\"\n",
    "        ids = []\n",
    "        for match in self.PAT.finditer(text):\n",
    "            token = match.group(0).encode(\"utf-8\")\n",
    "            for t in self.bpe(token):\n",
    "                if t in self.vocab_rev:\n",
    "                    ids.append(self.vocab_rev[t])\n",
    "        return ids\n",
    "\n",
    "    # 流式编码\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for line in iterable: # 逐行读取\n",
    "            for tid in self.encode(line):\n",
    "                yield tid # 对每一行中的内容诸葛返回 token id\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        byte_stream = b\"\".join(\n",
    "            self.vocab.get(i, b\"\\xef\\xbf\\xbd\") for i in ids\n",
    "        )\n",
    "        return byte_stream.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def token_to_id(self, token: str) -> int: # 将特殊token或普通token字符串 → 对应ID\n",
    "        b = token.encode(\"utf-8\")\n",
    "        return self.vocab_rev.get(b, -1)\n",
    "\n",
    "    def id_to_token(self, idx: int) -> str: # 将ID → 对应token字符串\n",
    "        return self.vocab.get(idx, b\"<?>\").decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8154ed0b-60a6-4fd8-851d-8b04d8bcdfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def init_worker(vocab_path, merges_path, special_tokens):\n",
    "    global _tokenizer # 每个子进程初始化自己的 tokenizer\n",
    "    _tokenizer = Tokenizer.from_files(vocab_path, merges_path, special_tokens)\n",
    "\n",
    "def encode_line(line: str): # 子进程执行的编码函数\n",
    "    global _tokenizer\n",
    "    if not line.strip():\n",
    "        return []\n",
    "    return _tokenizer.encode(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad978d34-606d-4df5-b807-78a966e9bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_encode_file_streaming(\n",
    "    input_txt_path: str,\n",
    "    output_tokens_path: str,\n",
    "    vocab_path: str = \"./bpeModel/vocab.json\",\n",
    "    merges_path: str = \"./bpeModel/merges.txt\",\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    num_workers: int = max(1, cpu_count() - 4),\n",
    "    chunk_size: int = 10000,  # 每批次读取多少行\n",
    "):\n",
    "    print(f\"Using {num_workers} processes to encode {input_txt_path}\")\n",
    "    print(f\"Streaming mode: reading {chunk_size} lines per batch\")\n",
    "\n",
    "    input_txt_path = Path(input_txt_path)\n",
    "    output_tokens_path = Path(output_tokens_path)\n",
    "    os.makedirs(output_tokens_path.parent, exist_ok=True)\n",
    "\n",
    "    tmp_path = output_tokens_path.with_suffix(\".tmp.npy\") # 缓存文件防止中途破坏影响最终结果\n",
    "    fp = np.memmap(tmp_path, mode=\"w+\", dtype=np.uint16, shape=(0,))  # 写入硬盘\n",
    "\n",
    "    total_tokens = 0\n",
    "    buffer = []\n",
    "\n",
    "    with Pool(\n",
    "        processes=num_workers,\n",
    "        initializer=init_worker,\n",
    "        initargs=(vocab_path, merges_path, special_tokens),\n",
    "    ) as pool, open(input_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        while True:\n",
    "            lines = [line for _, line in zip(range(chunk_size), f) if line.strip()]\n",
    "            if not lines:\n",
    "                break\n",
    "\n",
    "            for encoded in pool.imap(encode_line, lines, chunksize=128):\n",
    "                buffer.extend(encoded)\n",
    "\n",
    "            if buffer:\n",
    "                arr = np.array(buffer, dtype=np.uint16)\n",
    "                if total_tokens == 0:\n",
    "                    fp = arr\n",
    "                else:\n",
    "                    fp = np.concatenate((fp, arr))\n",
    "                total_tokens += len(arr)\n",
    "                buffer.clear()\n",
    "\n",
    "            print(f\"-> processed {total_tokens:,} tokens so far...\")\n",
    "\n",
    "    np.save(output_tokens_path, fp)\n",
    "    print(f\"Saved {total_tokens:,} tokens → {output_tokens_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "520b939c-11f1-4645-83fd-d711721a4451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Using 10 processes to encode ./datasets/TinyStories/train_with_eot.txt\n",
      "   Streaming mode: reading 10000 lines per batch\n",
      "  → processed 3,470,426 tokens so far...\n",
      "  → processed 7,178,279 tokens so far...\n",
      "  → processed 10,685,225 tokens so far...\n",
      "  → processed 14,427,430 tokens so far...\n",
      "  → processed 17,848,816 tokens so far...\n",
      "  → processed 21,445,993 tokens so far...\n",
      "  → processed 25,042,640 tokens so far...\n",
      "  → processed 28,454,082 tokens so far...\n",
      "  → processed 31,855,955 tokens so far...\n",
      "  → processed 35,347,123 tokens so far...\n",
      "  → processed 38,980,635 tokens so far...\n",
      "  → processed 42,701,691 tokens so far...\n",
      "  → processed 46,289,058 tokens so far...\n",
      "  → processed 50,039,497 tokens so far...\n",
      "  → processed 53,734,376 tokens so far...\n",
      "  → processed 57,509,188 tokens so far...\n",
      "  → processed 61,440,786 tokens so far...\n",
      "  → processed 64,976,791 tokens so far...\n",
      "  → processed 68,584,368 tokens so far...\n",
      "  → processed 72,320,727 tokens so far...\n",
      "  → processed 76,007,961 tokens so far...\n",
      "  → processed 79,574,931 tokens so far...\n",
      "  → processed 83,252,309 tokens so far...\n",
      "  → processed 87,014,137 tokens so far...\n",
      "  → processed 90,797,088 tokens so far...\n",
      "  → processed 94,404,634 tokens so far...\n",
      "  → processed 97,896,201 tokens so far...\n",
      "  → processed 101,593,901 tokens so far...\n",
      "  → processed 105,110,447 tokens so far...\n",
      "  → processed 108,844,681 tokens so far...\n",
      "  → processed 112,389,513 tokens so far...\n",
      "  → processed 115,874,280 tokens so far...\n",
      "  → processed 119,381,281 tokens so far...\n",
      "  → processed 123,034,806 tokens so far...\n",
      "  → processed 126,652,537 tokens so far...\n",
      "  → processed 130,308,017 tokens so far...\n",
      "  → processed 133,753,116 tokens so far...\n",
      "  → processed 137,193,319 tokens so far...\n",
      "  → processed 140,878,505 tokens so far...\n",
      "  → processed 144,509,189 tokens so far...\n",
      "  → processed 148,064,216 tokens so far...\n",
      "  → processed 151,671,865 tokens so far...\n",
      "  → processed 155,131,404 tokens so far...\n",
      "  → processed 158,651,165 tokens so far...\n",
      "  → processed 162,336,392 tokens so far...\n",
      "  → processed 165,959,363 tokens so far...\n",
      "  → processed 169,579,539 tokens so far...\n",
      "  → processed 173,435,358 tokens so far...\n",
      "  → processed 176,972,019 tokens so far...\n",
      "  → processed 180,534,193 tokens so far...\n",
      "  → processed 184,057,395 tokens so far...\n",
      "  → processed 187,635,772 tokens so far...\n",
      "  → processed 191,407,615 tokens so far...\n",
      "  → processed 194,938,296 tokens so far...\n",
      "  → processed 198,457,685 tokens so far...\n",
      "  → processed 202,212,497 tokens so far...\n",
      "  → processed 205,851,093 tokens so far...\n",
      "  → processed 209,483,096 tokens so far...\n",
      "  → processed 212,941,445 tokens so far...\n",
      "  → processed 216,460,694 tokens so far...\n",
      "  → processed 219,945,425 tokens so far...\n",
      "  → processed 223,523,517 tokens so far...\n",
      "  → processed 227,246,348 tokens so far...\n",
      "  → processed 230,908,623 tokens so far...\n",
      "  → processed 234,594,960 tokens so far...\n",
      "  → processed 238,166,064 tokens so far...\n",
      "  → processed 241,793,256 tokens so far...\n",
      "  → processed 245,491,295 tokens so far...\n",
      "  → processed 249,288,990 tokens so far...\n",
      "  → processed 253,219,245 tokens so far...\n",
      "  → processed 256,667,913 tokens so far...\n",
      "  → processed 260,427,905 tokens so far...\n",
      "  → processed 264,174,521 tokens so far...\n",
      "  → processed 267,956,649 tokens so far...\n",
      "  → processed 271,499,728 tokens so far...\n",
      "  → processed 275,222,465 tokens so far...\n",
      "  → processed 278,775,247 tokens so far...\n",
      "  → processed 282,278,689 tokens so far...\n",
      "  → processed 286,127,156 tokens so far...\n",
      "  → processed 289,588,578 tokens so far...\n",
      "  → processed 293,093,622 tokens so far...\n",
      "  → processed 296,413,301 tokens so far...\n",
      "  → processed 299,997,578 tokens so far...\n",
      "  → processed 303,355,428 tokens so far...\n",
      "  → processed 307,000,272 tokens so far...\n",
      "  → processed 310,393,250 tokens so far...\n",
      "  → processed 313,711,557 tokens so far...\n",
      "  → processed 317,254,258 tokens so far...\n",
      "  → processed 321,048,336 tokens so far...\n",
      "  → processed 324,694,514 tokens so far...\n",
      "  → processed 328,410,458 tokens so far...\n",
      "  → processed 331,968,738 tokens so far...\n",
      "  → processed 335,758,500 tokens so far...\n",
      "  → processed 339,297,521 tokens so far...\n",
      "  → processed 342,726,886 tokens so far...\n",
      "  → processed 346,251,933 tokens so far...\n",
      "  → processed 349,719,986 tokens so far...\n",
      "  → processed 353,567,395 tokens so far...\n",
      "  → processed 357,250,129 tokens so far...\n",
      "  → processed 360,847,687 tokens so far...\n",
      "  → processed 364,456,912 tokens so far...\n",
      "  → processed 367,930,385 tokens so far...\n",
      "  → processed 371,395,291 tokens so far...\n",
      "  → processed 375,091,209 tokens so far...\n",
      "  → processed 378,740,822 tokens so far...\n",
      "  → processed 382,383,232 tokens so far...\n",
      "  → processed 385,994,183 tokens so far...\n",
      "  → processed 389,430,262 tokens so far...\n",
      "  → processed 393,021,886 tokens so far...\n",
      "  → processed 396,706,775 tokens so far...\n",
      "  → processed 399,961,309 tokens so far...\n",
      "  → processed 403,510,608 tokens so far...\n",
      "  → processed 407,068,710 tokens so far...\n",
      "  → processed 410,489,138 tokens so far...\n",
      "  → processed 414,114,728 tokens so far...\n",
      "  → processed 417,719,297 tokens so far...\n",
      "  → processed 421,131,332 tokens so far...\n",
      "  → processed 424,667,534 tokens so far...\n",
      "  → processed 428,188,628 tokens so far...\n",
      "  → processed 431,911,530 tokens so far...\n",
      "  → processed 435,554,992 tokens so far...\n",
      "  → processed 438,941,917 tokens so far...\n",
      "  → processed 442,719,809 tokens so far...\n",
      "  → processed 446,187,223 tokens so far...\n",
      "  → processed 449,816,295 tokens so far...\n",
      "  → processed 453,470,825 tokens so far...\n",
      "  → processed 457,257,090 tokens so far...\n",
      "  → processed 461,066,373 tokens so far...\n",
      "  → processed 464,818,012 tokens so far...\n",
      "  → processed 468,433,822 tokens so far...\n",
      "  → processed 472,021,396 tokens so far...\n",
      "  → processed 475,515,287 tokens so far...\n",
      "  → processed 479,187,981 tokens so far...\n",
      "  → processed 482,521,475 tokens so far...\n",
      "  → processed 486,172,105 tokens so far...\n",
      "  → processed 489,731,153 tokens so far...\n",
      "  → processed 493,292,095 tokens so far...\n",
      "  → processed 497,087,335 tokens so far...\n",
      "  → processed 500,593,718 tokens so far...\n",
      "  → processed 504,379,863 tokens so far...\n",
      "  → processed 507,950,263 tokens so far...\n",
      "  → processed 511,455,643 tokens so far...\n",
      "  → processed 515,042,886 tokens so far...\n",
      "  → processed 518,424,065 tokens so far...\n",
      "  → processed 521,923,714 tokens so far...\n",
      "  → processed 525,380,278 tokens so far...\n",
      "  → processed 528,995,302 tokens so far...\n",
      "  → processed 532,382,736 tokens so far...\n",
      "  → processed 536,019,823 tokens so far...\n",
      "  → processed 539,559,382 tokens so far...\n",
      "  → processed 543,045,152 tokens so far...\n",
      "  → processed 546,696,850 tokens so far...\n",
      "  → processed 550,150,574 tokens so far...\n",
      "  → processed 553,740,936 tokens so far...\n",
      "  → processed 557,330,975 tokens so far...\n",
      "  → processed 560,860,827 tokens so far...\n",
      "  → processed 564,484,603 tokens so far...\n",
      "  → processed 567,998,009 tokens so far...\n",
      "  → processed 571,615,960 tokens so far...\n",
      "  → processed 575,178,229 tokens so far...\n",
      "  → processed 579,039,204 tokens so far...\n",
      "  → processed 582,615,069 tokens so far...\n",
      "  → processed 586,180,610 tokens so far...\n",
      "  → processed 589,753,752 tokens so far...\n",
      "  → processed 593,370,475 tokens so far...\n",
      "  → processed 597,048,260 tokens so far...\n",
      "  → processed 600,687,257 tokens so far...\n",
      "  → processed 604,063,190 tokens so far...\n",
      "  → processed 607,642,793 tokens so far...\n",
      "  → processed 611,261,398 tokens so far...\n",
      "  → processed 614,966,614 tokens so far...\n",
      "  → processed 618,541,038 tokens so far...\n",
      "  → processed 622,156,322 tokens so far...\n",
      "  → processed 625,695,410 tokens so far...\n",
      "  → processed 629,209,638 tokens so far...\n",
      "  → processed 632,836,013 tokens so far...\n",
      "  → processed 636,355,859 tokens so far...\n",
      "  → processed 640,029,072 tokens so far...\n",
      "  → processed 643,494,484 tokens so far...\n",
      "  → processed 646,849,581 tokens so far...\n",
      "  → processed 650,300,437 tokens so far...\n",
      "  → processed 653,799,230 tokens so far...\n",
      "  → processed 657,248,649 tokens so far...\n",
      "  → processed 661,062,626 tokens so far...\n",
      "  → processed 664,507,748 tokens so far...\n",
      "  → processed 668,084,364 tokens so far...\n",
      "  → processed 671,710,368 tokens so far...\n",
      "  → processed 675,659,353 tokens so far...\n",
      "  → processed 679,099,422 tokens so far...\n",
      "  → processed 682,572,386 tokens so far...\n",
      "  → processed 686,028,823 tokens so far...\n",
      "  → processed 689,619,443 tokens so far...\n",
      "  → processed 693,293,399 tokens so far...\n",
      "  → processed 696,981,552 tokens so far...\n",
      "  → processed 700,437,075 tokens so far...\n",
      "  → processed 704,172,450 tokens so far...\n",
      "  → processed 707,685,113 tokens so far...\n",
      "  → processed 711,398,982 tokens so far...\n",
      "  → processed 714,884,034 tokens so far...\n",
      "  → processed 718,517,485 tokens so far...\n",
      "  → processed 721,993,097 tokens so far...\n",
      "  → processed 725,669,015 tokens so far...\n",
      "  → processed 729,146,841 tokens so far...\n",
      "  → processed 732,657,927 tokens so far...\n",
      "  → processed 736,131,363 tokens so far...\n",
      "  → processed 739,685,518 tokens so far...\n",
      "  → processed 743,138,727 tokens so far...\n",
      "  → processed 746,914,319 tokens so far...\n",
      "  → processed 750,574,687 tokens so far...\n",
      "  → processed 754,532,646 tokens so far...\n",
      "  → processed 758,300,129 tokens so far...\n",
      "  → processed 761,867,318 tokens so far...\n",
      "    Saved 761,867,318 tokens → datasets/tokens_train.npy\n"
     ]
    }
   ],
   "source": [
    "# parallel_encode_file_streaming(\n",
    "#     input_txt_path=\"./datasets/TinyStories/train_with_eot.txt\",\n",
    "#     output_tokens_path=\"./datasets/tokens_train.npy\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065fb414-283a-4ae2-8050-028d8dff060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Using 12 processes to encode ./datasets/TinyStories/valid_with_eot.txt\n",
      "   Streaming mode: reading 10000 lines per batch\n",
      "  -> processed 3,298,494 tokens so far...\n",
      "  -> processed 6,922,095 tokens so far...\n",
      "  -> processed 7,670,696 tokens so far...\n",
      "    Saved 7,670,696 tokens → datasets/tokens_valid.npy\n"
     ]
    }
   ],
   "source": [
    "# parallel_encode_file_streaming(\n",
    "#     input_txt_path=\"./datasets/TinyStories/valid_with_eot.txt\",\n",
    "#     output_tokens_path=\"./datasets/tokens_valid.npy\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CS336)",
   "language": "python",
   "name": "cs336"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
