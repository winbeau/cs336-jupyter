{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2814a1-68a1-4dd7-a994-943575e8dcea",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE) Trainer\n",
    "- 文本预分词 | special token 处理  \n",
    "- 统计相邻 pair 频率 | 更新增量 \n",
    "- 输出 vocab.json 与 merges.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de1945-b17d-42ff-8851-03fa6f1ce2b5",
   "metadata": {},
   "source": [
    "### 问题抽象\n",
    "现有 $n$ 组( $n = 2 \\times 10^6 $ , 每组长度约 $120$ ~ $150$ )变长字符串 $str_i$ ($len(str_i) \\leq 20,~i \\in [0, \\, n)$)，总长度 $\\sum_{i = 0}^n len(str_i) \\leq 150 \\times 10 \\times 2 \\times 10^6 = 3 \\times 10^9$，\n",
    "\n",
    "$vocab$ 为 $token ~ id$ 对 $Unicode ~ byte(s)$ 的映射集合（也就是 $token$ 集合），初始化为 $[0, 256)$ 对应其十六进制数所代表的 $Unicode ~ byte$ (可以理解为 $ASCII ~ plus$) \n",
    "\n",
    "**持续执行如下操作：**\n",
    "\n",
    "对每个字符串 ***所有最长 $token$*** 进行两两结合统计频率：\n",
    "\n",
    "<aside>\n",
    "\n",
    "初始状态每个字符是一个 $token$\n",
    "\n",
    "`word` → `wo` : 1  `or` : 1  `rd` : 1\n",
    "\n",
    "> 什么是***最长 $token$*** ？如 `newest` ，`est` 已分配 $token ~ id$，我们仅对 `ne` `ew` `west` 处理\n",
    "> \n",
    "</aside>\n",
    "\n",
    "将频率最高的一个 $pair$ （若并列则取最高字典序）分配新的 $token ~ id$ \n",
    "\n",
    "比如上述 `word` 则新分配 `279` → `wo`\n",
    "\n",
    "**截止状态：**\n",
    "\n",
    "最终整个序列已经分配了 $token ~ id$（几乎不可能） 或 $token ~ id$ 的数目 = $vocab\\_size$\n",
    "\n",
    "$token ~ id \\leq vocab\\_size = 10^4$\n",
    "\n",
    "---\n",
    "\n",
    "#### 输入\n",
    "\n",
    "二维字符串数组\n",
    "\n",
    "#### 输出\n",
    "\n",
    "$vocab$ : 从 $token ~ id$ 到 $bytes$ 的映射\n",
    "\n",
    "$merges$ : 产生的 合并 $pair$（按创建顺序排列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d02a7ef-3951-4398-9d4e-ab30ee1ef74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440a60a4-bf1e-4c9c-a1c5-1b2d70d781d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize(text: str, pattern, special_pattern, special_lookup):\n",
    "    tokens = []\n",
    "    if special_pattern is not None: # 处理 <|endoftxt|>\n",
    "        parts = re.split(special_pattern, text)\n",
    "        matches = re.findall(special_pattern, text)\n",
    "        for i, part in enumerate(parts):\n",
    "            if part:\n",
    "                tokens.extend([m.group(0) for m in re.finditer(pattern, part)])\n",
    "            if i < len(matches):\n",
    "                tokens.append(matches[i])\n",
    "    else:\n",
    "        tokens = [m.group(0) for m in re.finditer(pattern, text)]\n",
    "\n",
    "    # 转成字节序列表 bytes([b]) 生成单个字节对象 | bytes(b) 生成整个字节序列\n",
    "    corpus = [[bytes([b]) for b in token.encode(\"utf-8\")] for token in tokens]\n",
    "    return corpus, len(corpus)\n",
    "\n",
    "def count_pairs(corpus, special_set):\n",
    "    pairs = Counter()\n",
    "    for word in corpus: # 词内合并\n",
    "        for a, b in zip(word, word[1:]): # 两两配对\n",
    "            if a not in special_set and b not in special_set:\n",
    "                pairs[(a, b)] += 1\n",
    "    return pairs\n",
    "\n",
    "def decrement_pair(pair_counts, pair): # 删除操作，防止内存爆掉\n",
    "    if pair_counts[pair] > 1:\n",
    "        pair_counts[pair] -= 1\n",
    "    else:\n",
    "        del pair_counts[pair] # 释放 counts[b'x']为 0 的空间，提高排序效率 | 减少内存堆积\n",
    "\n",
    "def apply_merge(corpus, pair_counts, merge_pair, special_set):\n",
    "    a, b = merge_pair\n",
    "    merged = a + b\n",
    "    for word in corpus: # 寻找连续 a-b pair 对，替换成 merged\n",
    "        i = 0\n",
    "        while i < len(word) - 1: # 滑动窗口\n",
    "            if word[i] == a and word[i + 1] == b:\n",
    "                left = word[i - 1] if i > 0 else None\n",
    "                right = word[i + 2] if i + 2 < len(word) else None\n",
    "                if left and left not in special_set:\n",
    "                    decrement_pair(pair_counts, (left, a))\n",
    "                    pair_counts[(left, merged)] += 1\n",
    "                if right and right not in special_set:\n",
    "                    decrement_pair(pair_counts, (b, right))\n",
    "                    pair_counts[(merged, right)] += 1\n",
    "                word[i : i + 2] = [merged] # 把两个元素替换为一个\n",
    "            i += 1\n",
    "    if merge_pair in pair_counts: # merge_pair = [a, b]\n",
    "        del pair_counts[merge_pair] # 释放内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ba5e70-f976-4b81-92ba-406979c77352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str],\n",
    "    output_dir: str = \"./bpeModel\",\n",
    "    PAT: str = r\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pattern = re.compile(PAT, re.UNICODE)\n",
    "\n",
    "    vocab = {i: bytes([i]) for i in range(256)} # 初始化 vocab\n",
    "    for tok in special_tokens:\n",
    "        vocab[len(vocab)] = tok.encode(\"utf-8\") # len(vocab) 作下标添加 special token\n",
    "\n",
    "    special_lookup = set(special_tokens) # str 匹配\n",
    "    special_set = set(tok.encode(\"utf-8\") for tok in special_tokens) # bytes 匹配\n",
    "    special_pattern = ( # 正则匹配\n",
    "        re.compile(\"|\".join(re.escape(t) for t in sorted(special_tokens, key=len, reverse=True)))\n",
    "        if special_tokens else None\n",
    "    )\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    corpus, n_stories = pretokenize(text, pattern, special_pattern, special_lookup)\n",
    "    print(f\"Loaded {n_stories} tokens\")\n",
    "\n",
    "    pair_counts = count_pairs(corpus, special_set)\n",
    "    merges = []\n",
    "    print(f\"Initial unique pairs: {len(pair_counts)}\")\n",
    "\n",
    "    while len(vocab) < vocab_size and pair_counts:\n",
    "        # item[0]: (a, b)  item[1]: freq\n",
    "        (a, b), freq = max(pair_counts.items(), key=lambda item: (item[1], item[0])) \n",
    "        vocab[len(vocab)] = a + b # 分配新 token\n",
    "        merges.append((a, b))\n",
    "        apply_merge(corpus, pair_counts, (a, b), special_set)\n",
    "        if len(merges) % 100 == 0:\n",
    "            print(f\"Step {len(merges)}, merged {a+b} freq={freq}\")\n",
    "\n",
    "    vocab_out = {k: v.decode(\"utf-8\", errors=\"ignore\") for k, v in vocab.items()}\n",
    "    with open(os.path.join(output_dir, \"vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab_out, f, ensure_ascii=False, indent=2)\n",
    "    with open(os.path.join(output_dir, \"merges.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for a, b in merges:\n",
    "            f.write(f\"{a.decode('utf-8', 'ignore')} {b.decode('utf-8', 'ignore')}\\n\")\n",
    "\n",
    "    print(f\"BPE Training done: {len(vocab)} tokens, {len(merges)} merges.\")\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1a5b37-06cd-4c8a-8070-f5c7916d68c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4554143 tokens\n",
      "Initial unique pairs: 1048\n",
      "Step 100, merged b'ad' freq=28155\n",
      "Step 200, merged b'fu' freq=12244\n",
      "Step 300, merged b'ited' freq=6999\n",
      "Step 400, merged b' outside' freq=4795\n",
      "Step 500, merged b' sorry' freq=3499\n",
      "Step 600, merged b'lly' freq=2692\n",
      "Step 700, merged b'xt' freq=2078\n",
      "Step 800, merged b' give' freq=1728\n",
      "Step 900, merged b' curi' freq=1453\n",
      "Step 1000, merged b' having' freq=1232\n",
      "Step 1100, merged b' swim' freq=1063\n",
      "Step 1200, merged b' filled' freq=929\n",
      "Step 1300, merged b'ons' freq=792\n",
      "Step 1400, merged b' treasure' freq=691\n",
      "Step 1500, merged b'fort' freq=619\n",
      "Step 1600, merged b' plac' freq=542\n",
      "Step 1700, merged b' Wh' freq=490\n",
      "Step 1800, merged b' Mittens' freq=441\n",
      "Step 1900, merged b' comfort' freq=397\n",
      "Step 2000, merged b' which' freq=357\n",
      "Step 2100, merged b' teeth' freq=328\n",
      "Step 2200, merged b' meant' freq=298\n",
      "Step 2300, merged b' sold' freq=272\n",
      "Step 2400, merged b' lit' freq=251\n",
      "Step 2500, merged b'iff' freq=231\n",
      "Step 2600, merged b' become' freq=218\n",
      "Step 2700, merged b' bags' freq=203\n",
      "Step 2800, merged b' beak' freq=192\n",
      "Step 2900, merged b' knows' freq=182\n",
      "Step 3000, merged b' falls' freq=174\n",
      "Step 3100, merged b' sadly' freq=165\n",
      "Step 3200, merged b' yet' freq=155\n",
      "Step 3300, merged b'ric' freq=148\n",
      "Step 3400, merged b' intellig' freq=141\n",
      "Step 3500, merged b' roof' freq=134\n",
      "Step 3600, merged b' pushing' freq=128\n",
      "Step 3700, merged b' dishes' freq=123\n",
      "Step 3800, merged b'ses' freq=115\n",
      "Step 3900, merged b' leaned' freq=110\n",
      "Step 4000, merged b'seum' freq=104\n",
      "Step 4100, merged b' greedy' freq=99\n",
      "Step 4200, merged b'aggie' freq=94\n",
      "Step 4300, merged b' flies' freq=90\n",
      "Step 4400, merged b' sque' freq=85\n",
      "Step 4500, merged b' hurting' freq=81\n",
      "Step 4600, merged b' happens' freq=77\n",
      "Step 4700, merged b' boot' freq=74\n",
      "Step 4800, merged b' langu' freq=70\n",
      "Step 4900, merged b' challenge' freq=66\n",
      "Step 5000, merged b' lemonade' freq=62\n",
      "Step 5100, merged b' language' freq=58\n",
      "Step 5200, merged b' Lena' freq=55\n",
      "Step 5300, merged b' cheering' freq=52\n",
      "Step 5400, merged b' onions' freq=48\n",
      "Step 5500, merged b'expected' freq=45\n",
      "Step 5600, merged b'iggy' freq=42\n",
      "Step 5700, merged b' towels' freq=40\n",
      "Step 5800, merged b' rainbows' freq=38\n",
      "Step 5900, merged b' fairies' freq=36\n",
      "Step 6000, merged b' awake' freq=34\n",
      "Step 6100, merged b' accomplishment' freq=32\n",
      "Step 6200, merged b' Amelia' freq=30\n",
      "Step 6300, merged b' det' freq=28\n",
      "Step 6400, merged b' Everybody' freq=27\n",
      "Step 6500, merged b' siblings' freq=25\n",
      "Step 6600, merged b' lighter' freq=24\n",
      "Step 6700, merged b' displayed' freq=23\n",
      "Step 6800, merged b'tuce' freq=21\n",
      "Step 6900, merged b'ppa' freq=20\n",
      "Step 7000, merged b' Robbie' freq=20\n",
      "Step 7100, merged b' gracefully' freq=19\n",
      "Step 7200, merged b' knights' freq=18\n",
      "Step 7300, merged b' postman' freq=17\n",
      "Step 7400, merged b'Nina' freq=16\n",
      "Step 7500, merged b' Fiona' freq=16\n",
      "Step 7600, merged b' merma' freq=15\n",
      "Step 7700, merged b'filled' freq=14\n",
      "Step 7800, merged b' heaven' freq=14\n",
      "Step 7900, merged b'ippy' freq=13\n",
      "Step 8000, merged b' engines' freq=13\n",
      "Step 8100, merged b'Always' freq=12\n",
      "Step 8200, merged b' crowns' freq=12\n",
      "Step 8300, merged b'His' freq=11\n",
      "Step 8400, merged b' dump' freq=11\n",
      "Step 8500, merged b'ormation' freq=10\n",
      "Step 8600, merged b' movements' freq=10\n",
      "Step 8700, merged b' Tigers' freq=10\n",
      "Step 8800, merged b'Try' freq=9\n",
      "Step 8900, merged b' man\\xc3\\xa2' freq=9\n",
      "Step 9000, merged b' Swim' freq=9\n",
      "Step 9100, merged b'lo' freq=8\n",
      "Step 9200, merged b' rotted' freq=8\n",
      "Step 9300, merged b' disagreement' freq=8\n",
      "Step 9400, merged b' Greg' freq=8\n",
      "Step 9500, merged b'fived' freq=7\n",
      "Step 9600, merged b' stiff' freq=7\n",
      "Step 9700, merged b' hoot' freq=7\n",
      "BPE Training done: 10000 tokens, 9743 merges.\n"
     ]
    }
   ],
   "source": [
    "vocab, merges = train_bpe(\n",
    "    input_path=\"./datasets/TinyStories/valid.txt\",\n",
    "    vocab_size=10000,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    "    output_dir=\"./bpeModel\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CS336)",
   "language": "python",
   "name": "cs336"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
