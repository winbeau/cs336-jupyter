{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c31f6e-b0f4-4d53-90f5-c3a067ab601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import import_ipynb\n",
    "from TransformerLM import TransformerLM\n",
    "from BPE_Tokenizer import Tokenizer\n",
    "from TrainFunctions import load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a740ba9-059a-4428-a23a-8baf7dca0e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLM(\n",
       "  (token_emb): Embedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "      (attn): MultiHeadSelfAttention(\n",
       "        (W_Q): Linear()\n",
       "        (W_K): Linear()\n",
       "        (W_V): Linear()\n",
       "        (W_O): Linear()\n",
       "        (rope): RoPE()\n",
       "      )\n",
       "      (ffn): SwiGLU(\n",
       "        (W1): Linear()\n",
       "        (W2): Linear()\n",
       "        (W3): Linear()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (out): Linear()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tok = Tokenizer.from_files( # 加载 tokenizer\n",
    "    \"./bpeModel/vocab.json\",\n",
    "    \"./bpeModel/merges.txt\",\n",
    "    special_tokens=[\"<|endoftext|>\"]\n",
    ")\n",
    "ckpt_path = \"./TrainLoopFiles/checkpoints_localhost/checkpoint_final.pt\"\n",
    "\n",
    "# 模型超参\n",
    "vocab_size=10000 \n",
    "num_layers=6\n",
    "d_model=512\n",
    "num_heads=8\n",
    "max_seq_len=512\n",
    "\n",
    "# 初始化模型并加载权重\n",
    "model = TransformerLM(vocab_size, d_model, num_heads, num_layers, max_seq_len)\n",
    "\n",
    "load_checkpoint(model, optimizer=None, src_path=ckpt_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27112e4b-05ed-470a-8107-2aed60252988",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt, \n",
    "    max_new_tokens=128, \n",
    "    temperature=0.8, \n",
    "    top_p=0.9\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device # 自动模型所在检测设备\n",
    "    ids = tokenizer.encode(prompt)\n",
    "    if len(ids) == 0:\n",
    "        ids = [0]\n",
    "    tokens = torch.tensor([ids], device=device)  # shape [1, T]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(tokens)[:, -1, :] # [b, t, v] 对 t 只取最后一个位置的预测\n",
    "        \"\"\"数值清理\"\"\"\n",
    "        logits = torch.nan_to_num(logits, nan=0.0, posinf=10.0, neginf=-10.0)\n",
    "        logits = torch.clamp(logits, -20, 20) # 限制极值\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        probs = torch.nan_to_num(probs, nan=0.0)\n",
    "\n",
    "        \"\"\"top-p 采样\"\"\"\n",
    "        sorted_p, sorted_i = torch.sort(probs, descending=True)\n",
    "        cum = torch.cumsum(sorted_p, dim=-1) # 累计概率超过 top_p 时截断\n",
    "        mask = cum > top_p\n",
    "        sorted_p[mask] = 0\n",
    "\n",
    "        if sorted_p.sum() <= 0 or not torch.isfinite(sorted_p).all():\n",
    "            next_id = torch.argmax(probs, dim=-1) # 和 0 或 NaN 则贪心选择\n",
    "        else:\n",
    "            sorted_p /= sorted_p.sum() # 按概率分布随机采样\n",
    "            sampled = torch.multinomial(sorted_p, 1)\n",
    "            next_id = sorted_i.gather(-1, sampled).squeeze(1)\n",
    "\n",
    "        \"\"\"拼接修正\"\"\"\n",
    "        tokens = torch.cat([tokens, next_id.unsqueeze(1)], dim=1)\n",
    "\n",
    "        if next_id.item() == tokenizer.token_to_id(\"<|endoftext|>\"):\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(tokens[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4379332-0cf6-460b-886e-50e8414cf616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Text ===\n",
      "Once upon a time, there was a little girl named Sam. They loved to clean in the park. They looked at the door and helped it. She started to play with her ball.  They were the fire toy different and saw a big home. They had fun on the floor, and saw the wanted to do the ra\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(model, tok, \"Once upon a time\", max_new_tokens=256)\n",
    "print(\"=== Generated Text ===\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbaf47-bd28-4e34-bcb5-8ba49cb17805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CS336)",
   "language": "python",
   "name": "cs336"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
