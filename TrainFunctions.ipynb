{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e06ff8-590b-4331-9efa-b176542d6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Tuple, Iterable\n",
    "import math, os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e6d51-da2f-4ac1-90ee-c9f952b693fc",
   "metadata": {},
   "source": [
    "## 1-交叉熵 | 数值稳定\n",
    "- 交叉熵(Cross Entropy Loss) 在所有 Transformer Block 训练之后，对 其 `[batch_size, seq_len, vocab_size]` 的未归一化分数进行计算损失\n",
    "\n",
    "- 模型是为了预测下一个 token 故输出张量最后一维 softmax 后才是真正的模型结果\n",
    "\n",
    "- 函数语义: 求 真值位置的 soft 得分 在 所有位置 soft 得分之和的比例，再取对数、负数\n",
    "\n",
    "- 真值得分越高，占比越大，取对数越大，取负数后越小 -> 预测错误损失会越大\n",
    "\n",
    "> 对于每个 batch 样本，模型输出形状是`logits: (seq_len, vocab_size)`\n",
    "> 对序列中每个位置，取该位置对应的 `logits[i]`(一个长度为 vocab_size 的向量)，\n",
    "> 经过 softmax 得到一个分布即序列中 下一个token 的概率分布\n",
    "\n",
    "### 如何做到数值稳定?\n",
    "设最后一维每 vocab 每个 token 得分向量为 sc，真值 y，则对应每一个 seq_len 的 i 位置: \n",
    "\n",
    "$$\n",
    "loss = -log\\frac{e^{sc_y}}{\\sum_j^Ve^{sc_j}} = log\\sum_j^Ve^{sc_j} - sc_y\n",
    "$$\n",
    "\n",
    "数值稳定问题? 分母求和可能会爆掉，尝试对所有元素减去最大值，softmax单调性不变\n",
    "$$\n",
    "log\\sum_j^Ve^{sc_j} = log\\sum_j^Ve^{sc_j - sc_m} + sc_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc045f4c-f03e-4529-a011-2a02cb248fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, targets): # logits[b, l, v] 推理结果 targets[b, l] 目标结果\n",
    "    logits_f32 = logits.to(torch.float32)\n",
    "    \n",
    "    max_logits, _ = torch.max(logits_f32, dim=-1, keepdim=True) # [b, l, v] -> [n, l, 1]\n",
    "    shifted = logits_f32 - max_logits # 广播机制 [b, l, v]\n",
    "    # 支持任意批维度\n",
    "    sum_exp = torch.sum(torch.exp(shifted), dim=-1, keepdim=False) # [b, l] 作为 seq_len 的每一token预测的soft总分\n",
    "    log_sum_exp = torch.log(sum_exp) + max_logits.squeeze(-1) # max_logits: [b, l, 1] -> [b, l]\n",
    "\n",
    "    # targets.unsqueeze(-1) [b, l] -> [b, l, 1]  logits: [b, l, v] take_along_dim ->[b, l, 1]    \n",
    "    true_logits = torch.take_along_dim(logits_f32, targets.unsqueeze(-1), dim=-1).squeeze(-1) # [b, l]\n",
    "\n",
    "    tmp = log_sum_exp - true_logits\n",
    "    return tmp.mean() # 对所有元素求平均"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ebb37d-5549-4205-b31e-3d7459da79dd",
   "metadata": {},
   "source": [
    "## 2-AdamW | Adam + 权重衰退\n",
    "\n",
    "随机梯度下降 (SGD) 存在如下问题:\n",
    "- 若梯度波动较大(噪声), 梯度下降的过程可能会在一个 \"山谷\" 来回波动\n",
    "- 梯度较大时，参数更新的步长很大，容易忽略掉很多信息；梯度较小时，参数更新的步长很小，效率变慢/时间被拖长\n",
    "  不同的参数的梯度不同，参数更新不同步\n",
    "- 缺少正则化操作容易过拟合\n",
    "\n",
    "### Adam 如何做?\n",
    "\n",
    "- 动量法: 记录之前的梯度值，用超参数 $\\beta$ 调整当前梯度的权重，为梯度下降增加 \"惯性\" 约束\n",
    "- 学习率自适应: 通过计算平方梯度的平均值(动量法下的梯度均方)来对学习率进行一定的缩放:\n",
    "  梯度较大，梯度均方较大，学习率缩小；梯度较小，梯度均方较大，学习率增大\n",
    "\n",
    "### 权重衰退如何做?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929fc22-884c-4748-83b2-8f41f867dc0b",
   "metadata": {},
   "source": [
    "### AdamW 具体实现中的语义\n",
    "\n",
    "1. 动量法:\n",
    "    - $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t ~~~ | ~~~ \\text{Expoential Moving Average}$ -> `exp_avg`\n",
    "      控制梯度下降方向\n",
    "    - $v_t = \\beta_2v_{t-1} + (1 - \\beta_2)g_t^2 ~~~ | ~~~ \\text{Exponential Average of Squared Gradients}$ -> `exp_avg_sq`\n",
    "      控制梯度下降幅度，选平方平均是为了去线性\n",
    "    - $\\text{time step}$ -> `step`\n",
    "\n",
    "> ps:\n",
    "> 前几步进行滑动平均计算动量时 会出现 bias bias 问题(初始为0，前几步平均值偏小)\n",
    ">\n",
    "> Adam 做修正(思想: 初始时扩大，次数累计后可忽略，并且是一个平滑的过渡):\n",
    "> \n",
    "> $\\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t}$， $\\hat{v_t} = \\frac{v_t}{1 - \\beta_2^t}$\n",
    "> \n",
    "2. 学习率自适应 $\\alpha_t \\frac{1}{\\sqrt{\\hat{v_t}} + \\epsilon}$\n",
    "3. **计算过程优化**:\n",
    "\n",
    "> 一阶动量(`exp_avg`)控制方向 ✨ 平滑修正 $\\frac{1}{1 - \\beta_1^t}$、$\\frac{1}{1 - \\beta_2^t}$ ✨ 二阶动量(`exp_avg_sq`适应学习率)\n",
    "> $$\n",
    "\\theta_t = \\theta_{t - 1} - \\alpha \\cdot \\frac{\\frac{m_t}{1 - \\beta_1^t}}{\\sqrt{\\frac{v_t}{1 - \\beta_2^t}} + \\epsilon}\n",
    "$$\n",
    "> 整理得：\n",
    "> $$\n",
    "\\frac{\\frac{m_t}{1 - \\beta_1^t}}{\\sqrt{\\frac{v_t}{1 - \\beta_2^t}} + \\epsilon}\n",
    "= \\frac{m_t}{\\sqrt{v_t} + \\epsilon \\sqrt{1 - \\beta_2^t}} \\cdot \n",
    "\\frac{\\sqrt{1 - \\beta_2^t}}{1 - \\beta_1^t}\n",
    "$$\n",
    "> 得到偏执修正系数 $\\frac{\\sqrt{1 - \\beta_2^t}}{1 - \\beta_1^t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c12388-aa33-497a-a4ee-e3358ea91f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(torch.optim.Optimizer):  \n",
    "    def __init__(\n",
    "        self, \n",
    "        params, \n",
    "        lr: float = 1e-3, \n",
    "        betas: Tuple[float, float] = (0.9, 0.95),\n",
    "        eps: float = 1e-8, \n",
    "        weight_decay=0.0\n",
    "    ): # betas 动量超参\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if eps < 0:\n",
    "            raise ValueError(f\"Invalid eps: {eps}\")\n",
    "        if weight_decay < 0:\n",
    "            raise ValueError(f\"Invalid weight_decay: {weight_decay}\")\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay) # 默认参数\n",
    "        super().__init__(params, defaults) # torch.optim.Optimizer 会自创建 self.param_groups\n",
    "\n",
    "    @torch.no_grad() # 禁止梯度追踪\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None: # closure 函数重新计算 loss + backward\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups: # 遍历所有组 每一组是一层的超参数\n",
    "            lr = group[\"lr\"]\n",
    "            beta1, beta2 = group[\"betas\"]\n",
    "            eps = group[\"eps\"]\n",
    "            wd = group[\"weight_decay\"]\n",
    "\n",
    "            for p in group[\"params\"]: # group[\"params\"] : 需要被优化的参数(权重)\n",
    "                if p.grad is None: \n",
    "                    continue \n",
    "                grad = p.grad.data\n",
    "                state = self.state[p] # 提取当前操作参数(权重)进行计算\n",
    "                if len(state) == 0: # 初始化动量参数\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data) \n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg = state[\"exp_avg\"]\n",
    "                exp_avg_sq = state[\"exp_avg_sq\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "                t = state[\"step\"]\n",
    "\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=(1 - beta1))\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=(1 - beta2))\n",
    "\n",
    "                bias_cor1 = 1 - beta1 ** t\n",
    "                bias_cor2 = 1 - beta2 ** t \n",
    "\n",
    "                step_size = lr * math.sqrt(bias_cor2) / bias_cor1 # 实际步长 | 简化除法计算\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(eps) # 归一化计算滑动平均 v_t | denominator 分母\n",
    "\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size) \n",
    "\n",
    "                if wd != 0: \n",
    "                    p.data.add_(p.data, alpha=-lr * wd) # 超参数 wd: 让所有参数每步都向 0 缩一点\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e900626-90c5-4e33-abc8-a5aedfe8fbdc",
   "metadata": {},
   "source": [
    "## 3-余弦退火\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "lrs = [cosine_lr_schedule(t, 1e-3, 1e-5, 500, 5000) for t in range(6000)]\n",
    "plt.plot(lrs)\n",
    "plt.title(\"Cosine Annealing LR Schedule with Warmup\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed165bf-f328-45c1-b4b4-7af4fe1166f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_lr_schedule(\n",
    "    t: int,\n",
    "    lr_max: float, \n",
    "    lr_min: float, \n",
    "    warmup_iters: int, \n",
    "    cosine_iters: int,\n",
    ") -> float: \n",
    "    if t < warmup_iters: # 线性预热\n",
    "        return lr_max * (t / max(1, warmup_iters))\n",
    "    if t <= cosine_iters: # 余弦退火\n",
    "        progress = (t - warmup_iters) / max(1, (cosine_iters - warmup_iters))\n",
    "        cosine_term = 0.5 * (1.0 + math.cos(math.pi * progress)) # 上平移 1.0 乘 0.5 归一化\n",
    "        return lr_min + (lr_max - lr_min) * cosine_term\n",
    "    return lr_min # 退火结束 保持最小学习率   \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "# set_matplotlib_formats('svg')  # svg 绘制\n",
    "\n",
    "# lrs = [cosine_lr_schedule(t, 1e-3, 1e-5, 500, 5000) for t in range(6000)]\n",
    "# plt.plot(lrs)\n",
    "# plt.title(\"Cosine Annealing LR Schedule with Warmup\")\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c859626-14a7-4504-9b9d-009253a356de",
   "metadata": {},
   "source": [
    "## 4-梯度裁剪\n",
    "通过计算全局L2范数 若它大于 max_norm 则按比例缩放，使新范数不超过 max_norm\n",
    "\n",
    "```python\n",
    "loss = cross_entropy_loss(...)\n",
    "loss.backward()           # 1️-生成梯度\n",
    "clip_gradients(params)    # 2-梯度裁剪（修正梯度）\n",
    "optimizer.step()          # 3️-使用被裁剪的梯度更新参数\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d11d24ac-bcc5-4c37-aea1-da720763db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clip_gradients(\n",
    "    params: Iterable[torch.nn.Parameter], \n",
    "    max_norm: float, \n",
    "    eps: float = 1e-6\n",
    "): \n",
    "    total_norm_sq = 0.0 \n",
    "    for p in params: # 计算 L2 范数\n",
    "        if p.grad is not None: \n",
    "            total_norm_sq += float(torch.sum(p.grad.data.to(torch.float32) ** 2))\n",
    "    total_norm = math.sqrt(total_norm_sq)\n",
    "\n",
    "    if total_norm > max_norm: \n",
    "        scale = max_norm / (total_norm + eps)\n",
    "        for p in params: \n",
    "            if p.grad is not None: \n",
    "                p.grad.data.mul_(scale)\n",
    "\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94decf2-52ae-4801-9199-b8d18c5b7ed5",
   "metadata": {},
   "source": [
    "## 5-随机采样\n",
    "\n",
    "2.2 M 个120-150词的小故事，随机截取连续 token 片段会把一个完整得小故事切断，但不会对模型训练造成问题\n",
    "\n",
    "`get_batch` 随机截取时:\n",
    "- 片段可能刚好落在故事中\n",
    "- 可能会跨过  `<|endoftext|>`\n",
    "- 模型会学习到——`<|endoftext|>` 是故事结尾\n",
    "### 借助 NumPy 库切片(左闭右开)\n",
    "x\\[i : i + m\\] -> input <br>\n",
    "x\\[i + 1 : i + 1 + m\\] -> output\n",
    "\n",
    "### 采用随机采样 而不是 epoch 式遍历\n",
    "- 不需要记录位置或状态；\n",
    "- 每步梯度更新都独立抽样；\n",
    "- 与随机梯度下降（SGD / AdamW）的假设完全一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "774aa6e1-f4db-4348-9eba-82a5355a16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size, context_len, device=\"cuda\"): \n",
    "    n = len(data) \n",
    "    starts = np.random.randint(0, n - context_len - 1, size=(batch_size,))\n",
    "    # 由于预测下一个 token  取 n - context_len - 1 位置作为样本起点\n",
    "\n",
    "    x_batch = np.stack([data[i : i + context_len] for i in starts])\n",
    "    y_batch = np.stack([data[i + 1 : i + 1 + context_len] for i in starts])\n",
    "\n",
    "    input_batch = torch.tensor(x_batch, dtype=torch.long, device=device)\n",
    "    output_batch = torch.tensor(y_batch, dtype=torch.long, device=device)\n",
    "    return input_batch, output_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78663443-a89e-4808-ae02-9c1ed686e4f1",
   "metadata": {},
   "source": [
    "## 6-checkpoint 保存/读取\n",
    "- 保存模型权重、优化器状态、当前 step\n",
    "- 可以从 checkpoint 恢复训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d898f57-725f-4bd5-b982-b7b90e8627cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, iteration: int, out_path: str):\n",
    "    payload = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"iteration\": iteration,\n",
    "    }\n",
    "    torch.save(payload, out_path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, src_path: str) -> int:\n",
    "    payload = torch.load(src_path, map_location=\"cpu\", weights_only=True) # 确保即使保存用 GPU，CPU 环境依然可加载\n",
    "    model.load_state_dict(payload[\"model_state\"])\n",
    "    if optimizer is not None and \"optimizer_state\" in payload:\n",
    "        optimizer.load_state_dict(payload[\"optimizer_state\"])\n",
    "    iteration = payload.get(\"iteration\", 0) # 默认 0 \n",
    "    return iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf3ba5-8e1e-4840-a3da-485433a982a5",
   "metadata": {},
   "source": [
    "## 7-加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b95d9553-418a-41e2-8657-f9d8ad673399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token_dataset(path: str, mmap: bool = True):\n",
    "    if mmap: # 使用内存映射模式，只在访问时加载数据\n",
    "        arr = np.load(path, mmap_mode=\"r\")\n",
    "    else:\n",
    "        arr = np.load(path)\n",
    "    print(f\"[load_token_dataset] Loaded {path}, shape={arr.shape}, dtype={arr.dtype}\")\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d0bcf-964b-49d3-a048-cbc7fd23fd7c",
   "metadata": {},
   "source": [
    "## 8-TrainingConfig 集中管理超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a94cc56a-f56a-4d50-8bb9-8715e0d1d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # === Data / batching ===\n",
    "    batch_size: int = 32\n",
    "    context_len: int = 256\n",
    "    vocab_size: int = 10000  # tokenizer vocab size\n",
    "\n",
    "    # === Model architecture ===\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 16\n",
    "    d_model: int = 1024\n",
    "    d_ff: int = field(default=None)  # 默认按 8/3*d_model 计算\n",
    "    rope_theta: float = 10000.0\n",
    "\n",
    "    # === Training steps ===\n",
    "    total_steps: int = 10_000\n",
    "    log_every: int = 50\n",
    "    eval_every: int = 500\n",
    "    ckpt_every: int = 1000\n",
    "\n",
    "    # === Optimizer (AdamW) ===\n",
    "    lr_max: float = 3e-4\n",
    "    lr_min: float = 3e-5\n",
    "    betas: Tuple[float, float] = (0.9, 0.95)\n",
    "    eps: float = 1e-8\n",
    "    weight_decay: float = 0.1\n",
    "    grad_clip_norm: float = 1.0\n",
    "\n",
    "    # === Scheduler ===\n",
    "    warmup_iters: int = 200\n",
    "    cosine_iters: int = 10000  # after this, lr = lr_min\n",
    "\n",
    "    # === Misc ===\n",
    "    device: str = \"cuda\"\n",
    "    target_dir = \"./TrainLoopFiles\"\n",
    "    ckpt_dir: str = None\n",
    "    ckpt_save_prefix: str = \"checkpoint\"\n",
    "    log_dir: str = None\n",
    "    log_save_prefix: str = \"train\"\n",
    "    ckpt_path_to_resume: str = None\n",
    "\n",
    "    # === Derived attributes ===\n",
    "    def __post_init__(self): # 自动计算 d_ff, (8/3 × d_model, 且为64的倍数)\n",
    "        if self.d_ff is None:\n",
    "            multiple = 64\n",
    "            raw_dff = int((8 / 3) * self.d_model)\n",
    "            self.d_ff = multiple * ((raw_dff + multiple - 1) // multiple)\n",
    "        if self.target_dir is None:\n",
    "            self.target_dir = \".\"\n",
    "        if self.ckpt_dir is None: \n",
    "            self.ckpt_dir = os.path.join(self.target_dir, \"checkpoints\")\n",
    "        if self.log_dir is None: \n",
    "            self.log_dir = os.path.join(self.target_dir, \"logs\")\n",
    "    \n",
    "    @staticmethod # 静态成员函数\n",
    "    def cal_d_ff(d_m) -> int: # 计算 d_ff\n",
    "        multiple = 64\n",
    "        raw_dff = int((8 / 3) * d_m)\n",
    "        return multiple * ((raw_dff + multiple - 1) // multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59fbe4f8-3830-4ba1-85cd-51b775d95782",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainingConfig(ckpt_dir=\"./TrainLoopFiles/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f03ef0-c6eb-4e32-9ddb-bc3780773147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./TrainLoopFiles/test'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.ckpt_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6dd11a2-434d-4376-9b8b-8c4aac85a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg1 = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59cac87e-7f8a-4313-a944-3f4cb37f0256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./TrainLoopFiles/checkpoints\n"
     ]
    }
   ],
   "source": [
    "print(cfg1.ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b120b6-d343-42b3-9f7e-951b7868c7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CS336)",
   "language": "python",
   "name": "cs336"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
