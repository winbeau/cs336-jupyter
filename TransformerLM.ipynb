{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fcbdd6-4ed2-4341-bb15-c12d62c63c9b",
   "metadata": {},
   "source": [
    "# Transformer From 0 To 1\n",
    "1. Embedding | 语句 -> 词向量\n",
    "2. RMSNorm(Layer Normalization) | 层归一化-预归一化(pre-norm)\n",
    "3. Multi-Head-Attention:\n",
    "   - Linear(W_Q, W_K, W_V, W_O)\n",
    "   - RoPE | 旋转-位置信息\n",
    "   - softmax\n",
    "4. SwiGLU(FeedForward) | 激活函数-门控模块\n",
    "5. Linear | 输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb77fb8-8359-4588-a487-8660218a3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a16f1c-8c86-449a-825d-b4ff22dc8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module): # @ Embedding weight\n",
    "    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None): \n",
    "        super().__init__() # 使用 empty() 初始化形状，无需填充 0 更快\n",
    "        self.weight = nn.Parameter(torch.empty(num_embeddings, embedding_dim))\n",
    "        nn.init.trunc_normal_(self.weight, mean=0.0, std=1.0, a=-3.0, b=3.0)\n",
    "\n",
    "    def forward(self, token_ids): \n",
    "        return self.weight[token_ids] # 返回对应权重的索引行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0de0a09-cc43-4e55-9465-89bb9a5bf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module): # @ RMSNorm gain\n",
    "    def __init__(self, d_model, eps=1e-5, device=None, dtype=None): \n",
    "        super().__init__() \n",
    "        self.eps = eps # 防止除 0 的小常数\n",
    "        self.gain = nn.Parameter(nn.Parameter(torch.ones(d_model, device=device, dtype=dtype)))\n",
    "        # gain: 缩放系数，初值为 1 不改变输入\n",
    "\n",
    "    def forward(self, x): \n",
    "        x_dtype = x.dtype # 记录原来类型\n",
    "        x = x.to(torch.float32)\n",
    "        # 计算 self 在最后一维的 L2 范数\n",
    "        norm = x.norm(dim=-1, keepdim=True) / (x.size(-1) ** 0.5)\n",
    "        return (x / (norm + self.eps)) * self.gain.to(x_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8435009c-c0a7-4d6a-bd5a-e4157348ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module): \n",
    "    def __init__(self, d_in, d_out, device=None, dtype=None): \n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty(d_out, d_in, device=device, dtype=dtype))\n",
    "        xavier_std = (2 / (d_in + d_out)) ** 0.5 # Xavier 初始化 保持in/out方差稳定性\n",
    "        nn.init.trunc_normal_(self.W, mean=0.0, std=xavier_std, a=-3.0, b=3.0)\n",
    "\n",
    "    def forward(self, x): \n",
    "        return x @ self.W.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e470f3e-a4ea-4acd-a0ed-47a47fa6e5b5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{i,k} = i \\cdot \\frac{1}{\\Theta^{2k/d_k}} ~~~~~~~\n",
    "\\text{其中  freq\\_inv}_k = \\frac{1}{\\Theta^{2k/d_k}} ~~~\n",
    "\\Theta \\, 通常取 10^4\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "x_0' ~ x_1'\n",
    "\\end{bmatrix} ~ = ~ \n",
    "\\begin{bmatrix}\n",
    "x_0 ~~ x_1\n",
    "\\end{bmatrix} ~~\n",
    "\\begin{bmatrix}\n",
    "cos \\theta_k ~~ sin \\theta_k \\\\\n",
    "-sin \\theta_k ~~ cos \\theta_k\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac264cf5-c0c4-43c6-849d-c84202d8dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module): \n",
    "    def __init__(self, Theta, d_k, seq_len, device=None): \n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (Theta ** torch.arange(0, d_k, 2, device=device).float() / d_k) \n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,k->ik\", t, inv_freq)\n",
    "        self.register_buffer(\"cos\", torch.cos(freqs)) # 存为 buffer 后续查表\n",
    "        self.register_buffer(\"sin\", torch.sin(freqs))\n",
    "\n",
    "    def forward(self, x, positions): # x: [..., seq_len, d_k] position: [..., seq_len]\n",
    "        cos = self.cos[positions]\n",
    "        sin = self.sin[positions]\n",
    "        \"\"\"分隔奇偶 -> 求对应值 -> 按维度堆叠 -> 展平\"\"\"\n",
    "        x0, x1 = x[..., ::2], x[..., 1::2] # 把 x 拆分为偶数序列、奇数序列\n",
    "        x_rot = torch.stack((x0 * cos - x1 * sin, x0 * sin + x1 * cos), dim = -1)\n",
    "        return x_rot.flatten(-2) # 展开最后两维 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71c9c1-f4a9-427e-8ec0-50759b7bebb5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "h_1 = W_1 x \\\\ \n",
    "h_2 = W_3 x\n",
    "\\end{cases} ~~~ \\Rightarrow ~~~\n",
    "\\tilde{h} = SiLU(h_1) \\odot h_2 ~~~ \\Rightarrow ~~~\n",
    "y = W_2 \\tilde{h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3569874d-77a4-4be8-b90d-458b7401ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module): \n",
    "    def __init__(self, d_model, mul_of=64): # multiple_of 作倍数上取整 \n",
    "        super().__init__()\n",
    "        d_ff = int((8/3) * d_model)\n",
    "        d_ff = mul_of * ((d_ff + mul_of - 1) // mul_of)\n",
    "        self.W1 = Linear(d_model, d_ff)\n",
    "        self.W2 = Linear(d_ff, d_model)\n",
    "        self.W3 = Linear(d_model, d_ff) \n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.W2(F.silu(self.W1(x)) * self.W3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e536342-93d9-4163-ba3f-43c65e43bb6f",
   "metadata": {},
   "source": [
    "### 多头自注意力 + 因果mask + RoPE\n",
    "0. scaled dot-product attention 实现\n",
    "1. 输入 x: (B, T, C)\n",
    "2. 线性变换得到 Q_raw, K_raw, V_raw: (B, T, C)\n",
    "3. reshape+transpose → (B, h, T, d_head) 得到 Q/K/V\n",
    "4. 对 Q/K 用 RoPE 按位旋转，把位置信息注入\n",
    "5. 构造下三角 mask (T, T)，禁止看未来\n",
    "6. 对每个 head 做 scaled dot-product attention\n",
    "   得到 context_per_head: (B, h, T, d_head)\n",
    "7. 把多头拼回去：transpose -> view → (B, T, C)\n",
    "8. 通过输出投影 W_O，仍是 (B, T, C)，交给残差/后续FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19471af-b8e4-45ee-989e-d98cde24aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, dim=-1): \n",
    "    x = x - x.max(dim=dim, keepdim=True).values\n",
    "    exp_x = x.exp() \n",
    "    return exp_x / exp_x.sum(dim=dim, keepdim=True) # 广播机制\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None): \n",
    "    d_k = Q.size(-1) # Q: [..., seq_len, d_head]\n",
    "    scores = Q @ K.transpose(-2, -1) / (d_k ** 0.5) # K: [..., seq_len, d_head] 不能简单转置\n",
    "    if mask is not None: \n",
    "        scores = scores.masked_fill(~mask, float('-inf')) # 由于要做 softmax 掩码要取反\n",
    "    attn = softmax(scores, dim=-1) # 最后一个维度做 softmax\n",
    "    return attn @ V    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961d35b2-2e0b-4ce1-a258-5955ffdd5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module): \n",
    "    def __init__(self, d_model, num_heads, seq_len=1024, rope_theta=10000): \n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be a multiple of num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.W_Q = Linear(d_model, d_model)\n",
    "        self.W_K = Linear(d_model, d_model)\n",
    "        self.W_V = Linear(d_model, d_model)\n",
    "        self.W_O = Linear(d_model, d_model)\n",
    "        self.rope = RoPE(rope_theta, self.d_head, seq_len)\n",
    "\n",
    "    def forward(self, x): \n",
    "        B, T, C = x.shape # T = seq_len C = d_model\n",
    "        positions = torch.arange(T, device=x.device)\n",
    "        \"\"\"Q @ KT -> [..., seq_len, seq_len] -> scaled_dot -> @ V -> [..., seq_len, d_head]\"\"\"\n",
    "        Q = self.W_Q(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        K = self.W_K(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        V = self.W_V(x).view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        Q = self.rope(Q, positions) # RoPE: (..., seq_len, d_k) (..., seq_len)\n",
    "        K = self.rope(K, positions)\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=torch.bool))\n",
    "        out = scaled_dot_product_attention(Q, K, V, mask=mask) # 因果下三角\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C) # 创建连续副本 concat\n",
    "        return self.W_O(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5d01c40-2b65-4794-877c-3569694cb9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module): \n",
    "    def __init__(self, d_model, num_heads): \n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model) # 门控有参数 存两份\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.ffn = SwiGLU(d_model)\n",
    "\n",
    "    def forward(self, x): # 残差连接 + 预归一化\n",
    "        x = x + self.attn(self.norm1(x)) # 多头注意力\n",
    "        x = x + self.ffn(self.norm2(x)) # FFN 前馈神经网络\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a59e495-c86e-4cf1-8482-48d4ff084bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module): \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, max_seq_len=1024): \n",
    "        super().__init__() \n",
    "        self.token_emb = Embedding(vocab_size, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, num_heads) for _ in range(num_layers)])\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.out = Linear(d_model, vocab_size) # 线性层输出\n",
    "\n",
    "    def forward(self, token_ids): \n",
    "        x = self.token_emb(token_ids)\n",
    "        for blk in self.blocks: \n",
    "            x = blk(x)\n",
    "        x = self.norm(x) # 由于 pre-norm 输出前要再进行一次 层归一化\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631c1c83-9c26-4857-b601-e8a98b5e655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 1000\n",
    "# model = TransformerLM(vocab_size, d_model=128, num_heads=8, num_layers=2).to(device)\n",
    "\n",
    "# x = torch.randint(0, vocab_size, (2, 16), device=device)\n",
    "# y = model(x)\n",
    "# print(y.shape)  # (batch, seq_len, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
