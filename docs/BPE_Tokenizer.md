# Tokenizer
- 加载已经训练好的词表`vocab.json`、合并规则`merges.txt`
- bpe函数 | 对陌生 str 通过已经训练的规则进行处理
- 编码文本 (string -> token IDs)
- 解码tokens (token IDs -> string)


```python
import json
import regex as re
from typing import Dict, List, Tuple, Iterable, Iterator
```


```python
class Tokenizer:
    def __init__(
        self,
        vocab: Dict[int, bytes],
        merges: List[Tuple[bytes, bytes]],
        special_tokens: List[str] | None = None,
    ):
        self.vocab = vocab
        self.vocab_rev = {v: k for k, v in vocab.items()}
        self.special_tokens = special_tokens or []

        # 编译正则
        self.PAT = re.compile(r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")

        # 预建 merge rank 表
        self.bpe_ranks = {pair: i for i, pair in enumerate(merges)}

    @classmethod
    def from_files(
        cls,
        vocab_path: str,
        merges_path: str,
        special_tokens: List[str] | None = None,
    ) -> "Tokenizer":
        with open(vocab_path, "r", encoding="utf-8") as f:
            vocab_data = json.load(f)
        vocab = {int(i): v.encode("utf-8") for i, v in vocab_data.items()}

        merges = []
        with open(merges_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                # 忽略注释和非法行
                if not line or line.startswith("#"):
                    continue
                parts = line.split()
                if len(parts) != 2:
                    continue  # 跳过不合规的行
                a, b = parts
                merges.append((a.encode(), b.encode()))

        return cls(vocab, merges, special_tokens)

    def get_pairs(self, tokens: List[bytes]): # 取相邻 token pair
        return {(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)}

    def bpe(self, token: bytes) -> List[bytes]: # 执行 Byte-Pair Encoding
        word = [bytes([b]) for b in token]  # byte-level 初始分词
        pairs = self.get_pairs(word)

        while pairs:# 找到当前可合并的最小 rank(最高频率)
            min_pair = min(
                pairs, key=lambda p: self.bpe_ranks.get(p, float("inf"))
            )
            if min_pair not in self.bpe_ranks:
                break

            new_word = []
            i = 0
            while i < len(word):
                if (
                    i < len(word) - 1
                    and word[i] == min_pair[0]
                    and word[i + 1] == min_pair[1]
                ):
                    new_word.append(word[i] + word[i + 1])
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            word = new_word
            pairs = self.get_pairs(word)

        return word
        
    def encode(self, text: str) -> List[int]:
        """Encode text → token ids"""
        ids = []
        for match in self.PAT.finditer(text):
            token = match.group(0).encode("utf-8")
            for t in self.bpe(token):
                if t in self.vocab_rev:
                    ids.append(self.vocab_rev[t])
        return ids

    # 流式编码
    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:
        for line in iterable: # 逐行读取
            for tid in self.encode(line):
                yield tid # 对每一行中的内容诸葛返回 token id

    def decode(self, ids: List[int]) -> str:
        byte_stream = b"".join(
            self.vocab.get(i, b"\xef\xbf\xbd") for i in ids
        )
        return byte_stream.decode("utf-8", errors="replace")

    def token_to_id(self, token: str) -> int: # 将特殊token或普通token字符串 → 对应ID
        b = token.encode("utf-8")
        return self.vocab_rev.get(b, -1)

    def id_to_token(self, idx: int) -> str: # 将ID → 对应token字符串
        return self.vocab.get(idx, b"<?>").decode("utf-8", errors="replace")
```


```python
import os, json, numpy as np
from pathlib import Path
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

def init_worker(vocab_path, merges_path, special_tokens):
    global _tokenizer # 每个子进程初始化自己的 tokenizer
    _tokenizer = Tokenizer.from_files(vocab_path, merges_path, special_tokens)

def encode_line(line: str): # 子进程执行的编码函数
    global _tokenizer
    if not line.strip():
        return []
    return _tokenizer.encode(line)
```


```python
def parallel_encode_file_streaming(
    input_txt_path: str,
    output_tokens_path: str,
    vocab_path: str = "./bpeModel/vocab.json",
    merges_path: str = "./bpeModel/merges.txt",
    special_tokens=["<|endoftext|>"],
    num_workers: int = max(1, cpu_count() - 4),
    chunk_size: int = 10000,  # 每批次读取多少行
):
    print(f"Using {num_workers} processes to encode {input_txt_path}")
    print(f"Streaming mode: reading {chunk_size} lines per batch")

    input_txt_path = Path(input_txt_path)
    output_tokens_path = Path(output_tokens_path)
    os.makedirs(output_tokens_path.parent, exist_ok=True)

    tmp_path = output_tokens_path.with_suffix(".tmp.npy") # 缓存文件防止中途破坏影响最终结果
    fp = np.memmap(tmp_path, mode="w+", dtype=np.uint16, shape=(0,))  # 写入硬盘

    total_tokens = 0
    buffer = []

    with Pool(
        processes=num_workers,
        initializer=init_worker,
        initargs=(vocab_path, merges_path, special_tokens),
    ) as pool, open(input_txt_path, "r", encoding="utf-8") as f:

        while True:
            lines = [line for _, line in zip(range(chunk_size), f) if line.strip()]
            if not lines:
                break

            for encoded in pool.imap(encode_line, lines, chunksize=128):
                buffer.extend(encoded)

            if buffer:
                arr = np.array(buffer, dtype=np.uint16)
                if total_tokens == 0:
                    fp = arr
                else:
                    fp = np.concatenate((fp, arr))
                total_tokens += len(arr)
                buffer.clear()

            print(f"-> processed {total_tokens:,} tokens so far...")

    np.save(output_tokens_path, fp)
    print(f"Saved {total_tokens:,} tokens → {output_tokens_path}")
```


```python
parallel_encode_file_streaming(
    input_txt_path="./datasets/TinyStories/train_with_eot.txt",
    output_tokens_path="./datasets/tokens_train.npy",
)
```

       Using 10 processes to encode ./datasets/TinyStories/train_with_eot.txt
       Streaming mode: reading 10000 lines per batch
      → processed 3,470,426 tokens so far...
      → processed 7,178,279 tokens so far...
      → processed 10,685,225 tokens so far...
      → processed 14,427,430 tokens so far...
      → processed 17,848,816 tokens so far...
      → processed 21,445,993 tokens so far...
      → processed 25,042,640 tokens so far...
      → processed 28,454,082 tokens so far...
      → processed 31,855,955 tokens so far...
      → processed 35,347,123 tokens so far...
      → processed 38,980,635 tokens so far...
      → processed 42,701,691 tokens so far...
      → processed 46,289,058 tokens so far...
      → processed 50,039,497 tokens so far...
      → processed 53,734,376 tokens so far...
      → processed 57,509,188 tokens so far...
      → processed 61,440,786 tokens so far...
      → processed 64,976,791 tokens so far...
      → processed 68,584,368 tokens so far...
      → processed 72,320,727 tokens so far...
      → processed 76,007,961 tokens so far...
      → processed 79,574,931 tokens so far...
      → processed 83,252,309 tokens so far...
      → processed 87,014,137 tokens so far...
      → processed 90,797,088 tokens so far...
      → processed 94,404,634 tokens so far...
      → processed 97,896,201 tokens so far...
      → processed 101,593,901 tokens so far...
      → processed 105,110,447 tokens so far...
      → processed 108,844,681 tokens so far...
      → processed 112,389,513 tokens so far...
      → processed 115,874,280 tokens so far...
      → processed 119,381,281 tokens so far...
      → processed 123,034,806 tokens so far...
      → processed 126,652,537 tokens so far...
      → processed 130,308,017 tokens so far...
      → processed 133,753,116 tokens so far...
      → processed 137,193,319 tokens so far...
      → processed 140,878,505 tokens so far...
      → processed 144,509,189 tokens so far...
      → processed 148,064,216 tokens so far...
      → processed 151,671,865 tokens so far...
      → processed 155,131,404 tokens so far...
      → processed 158,651,165 tokens so far...
      → processed 162,336,392 tokens so far...
      → processed 165,959,363 tokens so far...
      → processed 169,579,539 tokens so far...
      → processed 173,435,358 tokens so far...
      → processed 176,972,019 tokens so far...
      → processed 180,534,193 tokens so far...
      → processed 184,057,395 tokens so far...
      → processed 187,635,772 tokens so far...
      → processed 191,407,615 tokens so far...
      → processed 194,938,296 tokens so far...
      → processed 198,457,685 tokens so far...
      → processed 202,212,497 tokens so far...
      → processed 205,851,093 tokens so far...
      → processed 209,483,096 tokens so far...
      → processed 212,941,445 tokens so far...
      → processed 216,460,694 tokens so far...
      → processed 219,945,425 tokens so far...
      → processed 223,523,517 tokens so far...
      → processed 227,246,348 tokens so far...
      → processed 230,908,623 tokens so far...
      → processed 234,594,960 tokens so far...
      → processed 238,166,064 tokens so far...
      → processed 241,793,256 tokens so far...
      → processed 245,491,295 tokens so far...
      → processed 249,288,990 tokens so far...
      → processed 253,219,245 tokens so far...
      → processed 256,667,913 tokens so far...
      → processed 260,427,905 tokens so far...
      → processed 264,174,521 tokens so far...
      → processed 267,956,649 tokens so far...
      → processed 271,499,728 tokens so far...
      → processed 275,222,465 tokens so far...
      → processed 278,775,247 tokens so far...
      → processed 282,278,689 tokens so far...
      → processed 286,127,156 tokens so far...
      → processed 289,588,578 tokens so far...
      → processed 293,093,622 tokens so far...
      → processed 296,413,301 tokens so far...
      → processed 299,997,578 tokens so far...
      → processed 303,355,428 tokens so far...
      → processed 307,000,272 tokens so far...
      → processed 310,393,250 tokens so far...
      → processed 313,711,557 tokens so far...
      → processed 317,254,258 tokens so far...
      → processed 321,048,336 tokens so far...
      → processed 324,694,514 tokens so far...
      → processed 328,410,458 tokens so far...
      → processed 331,968,738 tokens so far...
      → processed 335,758,500 tokens so far...
      → processed 339,297,521 tokens so far...
      → processed 342,726,886 tokens so far...
      → processed 346,251,933 tokens so far...
      → processed 349,719,986 tokens so far...
      → processed 353,567,395 tokens so far...
      → processed 357,250,129 tokens so far...
      → processed 360,847,687 tokens so far...
      → processed 364,456,912 tokens so far...
      → processed 367,930,385 tokens so far...
      → processed 371,395,291 tokens so far...
      → processed 375,091,209 tokens so far...
      → processed 378,740,822 tokens so far...
      → processed 382,383,232 tokens so far...
      → processed 385,994,183 tokens so far...
      → processed 389,430,262 tokens so far...
      → processed 393,021,886 tokens so far...
      → processed 396,706,775 tokens so far...
      → processed 399,961,309 tokens so far...
      → processed 403,510,608 tokens so far...
      → processed 407,068,710 tokens so far...
      → processed 410,489,138 tokens so far...
      → processed 414,114,728 tokens so far...
      → processed 417,719,297 tokens so far...
      → processed 421,131,332 tokens so far...
      → processed 424,667,534 tokens so far...
      → processed 428,188,628 tokens so far...
      → processed 431,911,530 tokens so far...
      → processed 435,554,992 tokens so far...
      → processed 438,941,917 tokens so far...
      → processed 442,719,809 tokens so far...
      → processed 446,187,223 tokens so far...
      → processed 449,816,295 tokens so far...
      → processed 453,470,825 tokens so far...
      → processed 457,257,090 tokens so far...
      → processed 461,066,373 tokens so far...
      → processed 464,818,012 tokens so far...
      → processed 468,433,822 tokens so far...
      → processed 472,021,396 tokens so far...
      → processed 475,515,287 tokens so far...
      → processed 479,187,981 tokens so far...
      → processed 482,521,475 tokens so far...
      → processed 486,172,105 tokens so far...
      → processed 489,731,153 tokens so far...
      → processed 493,292,095 tokens so far...
      → processed 497,087,335 tokens so far...
      → processed 500,593,718 tokens so far...
      → processed 504,379,863 tokens so far...
      → processed 507,950,263 tokens so far...
      → processed 511,455,643 tokens so far...
      → processed 515,042,886 tokens so far...
      → processed 518,424,065 tokens so far...
      → processed 521,923,714 tokens so far...
      → processed 525,380,278 tokens so far...
      → processed 528,995,302 tokens so far...
      → processed 532,382,736 tokens so far...
      → processed 536,019,823 tokens so far...
      → processed 539,559,382 tokens so far...
      → processed 543,045,152 tokens so far...
      → processed 546,696,850 tokens so far...
      → processed 550,150,574 tokens so far...
      → processed 553,740,936 tokens so far...
      → processed 557,330,975 tokens so far...
      → processed 560,860,827 tokens so far...
      → processed 564,484,603 tokens so far...
      → processed 567,998,009 tokens so far...
      → processed 571,615,960 tokens so far...
      → processed 575,178,229 tokens so far...
      → processed 579,039,204 tokens so far...
      → processed 582,615,069 tokens so far...
      → processed 586,180,610 tokens so far...
      → processed 589,753,752 tokens so far...
      → processed 593,370,475 tokens so far...
      → processed 597,048,260 tokens so far...
      → processed 600,687,257 tokens so far...
      → processed 604,063,190 tokens so far...
      → processed 607,642,793 tokens so far...
      → processed 611,261,398 tokens so far...
      → processed 614,966,614 tokens so far...
      → processed 618,541,038 tokens so far...
      → processed 622,156,322 tokens so far...
      → processed 625,695,410 tokens so far...
      → processed 629,209,638 tokens so far...
      → processed 632,836,013 tokens so far...
      → processed 636,355,859 tokens so far...
      → processed 640,029,072 tokens so far...
      → processed 643,494,484 tokens so far...
      → processed 646,849,581 tokens so far...
      → processed 650,300,437 tokens so far...
      → processed 653,799,230 tokens so far...
      → processed 657,248,649 tokens so far...
      → processed 661,062,626 tokens so far...
      → processed 664,507,748 tokens so far...
      → processed 668,084,364 tokens so far...
      → processed 671,710,368 tokens so far...
      → processed 675,659,353 tokens so far...
      → processed 679,099,422 tokens so far...
      → processed 682,572,386 tokens so far...
      → processed 686,028,823 tokens so far...
      → processed 689,619,443 tokens so far...
      → processed 693,293,399 tokens so far...
      → processed 696,981,552 tokens so far...
      → processed 700,437,075 tokens so far...
      → processed 704,172,450 tokens so far...
      → processed 707,685,113 tokens so far...
      → processed 711,398,982 tokens so far...
      → processed 714,884,034 tokens so far...
      → processed 718,517,485 tokens so far...
      → processed 721,993,097 tokens so far...
      → processed 725,669,015 tokens so far...
      → processed 729,146,841 tokens so far...
      → processed 732,657,927 tokens so far...
      → processed 736,131,363 tokens so far...
      → processed 739,685,518 tokens so far...
      → processed 743,138,727 tokens so far...
      → processed 746,914,319 tokens so far...
      → processed 750,574,687 tokens so far...
      → processed 754,532,646 tokens so far...
      → processed 758,300,129 tokens so far...
      → processed 761,867,318 tokens so far...
        Saved 761,867,318 tokens → datasets/tokens_train.npy



```python
parallel_encode_file_streaming(
    input_txt_path="./datasets/TinyStories/valid_with_eot.txt",
    output_tokens_path="./datasets/tokens_valid.npy",
)
```

       Using 12 processes to encode ./datasets/TinyStories/valid_with_eot.txt
       Streaming mode: reading 10000 lines per batch
      -> processed 3,298,494 tokens so far...
      -> processed 6,922,095 tokens so far...
      -> processed 7,670,696 tokens so far...
        Saved 7,670,696 tokens → datasets/tokens_valid.npy

